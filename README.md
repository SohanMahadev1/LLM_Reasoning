# Fine-Tuning Phi 3.5 Model for Reasoning Capabilities with GRPO

## Overview

**LLM-to-Reasoning LLM** is a fine-tuning project that transforms the **phi 3.5 model** into a reasoning-specific large language model (LLM). The project utilizes the **GRPO (Gradient Policy Optimization)** method for training the model to perform complex reasoning tasks. It focuses on improving the model's ability to process, understand, and respond to reasoning problems by fine-tuning with **Unsloth**, a state-of-the-art fine-tuning methodology. For inference, **vLLM** was employed to ensure efficient, high-speed, and low-latency processing, while **Kaggle Notebook** was used for experimentation and inference demonstrations.

This project combines cutting-edge techniques in model training and fine-tuning to create an LLM optimized for reasoning tasks. It showcases expertise in advanced optimization algorithms, fine-tuning strategies, and deploying scalable AI models for real-world applications.

---

## Key Features

1. **GRPO for Training**:
   - Applied the **Gradient Policy Optimization (GRPO)** method to train the phi 3.5 model.
   - Ensured the model's reasoning capabilities were tailored to handle complex tasks effectively.

2. **Fine-Tuning with Unsloth**:
   - Leveraged the **Unsloth framework** to fine-tune the model for reasoning tasks.
   - Improved accuracy and generalization for a wide range of reasoning applications.

3. **Efficient Inference with vLLM**:
   - Integrated **vLLM**, enabling low-latency and scalable inference for reasoning tasks.
   - Achieved high performance while maintaining efficiency for real-time use cases.

4. **Kaggle Notebook for Inference**:
   - Deployed the trained model on **Kaggle Notebooks** to demonstrate inference capabilities.
   - Provided a user-friendly environment for testing and showcasing the model's reasoning abilities.

---

## Workflow

The project follows the following steps:

1. **Model Training**:
   - Used GRPO to train the phi 3.5 model.
   - Focused on optimizing its reasoning capabilities.

2. **Fine-Tuning**:
   - Fine-tuned the trained model using the Unsloth framework for increased performance.

3. **Inference Pipeline**:
   - Utilized vLLM for deploying scalable and efficient inference pipelines.

4. **Deployment and Testing**:
   - Used Kaggle Notebook for testing and showcasing the modelâ€™s reasoning results.

---

## Tools and Technologies

- **Phi 3.5 Model**: Pre-trained LLM used as the base model.
- **GRPO (Gradient Policy Optimization)**: Training strategy to enhance reasoning capabilities.
- **Unsloth**: Framework used for fine-tuning the model.
- **vLLM**: High-performance, low-latency framework for inference.
- **Kaggle Notebook**: Platform used for deployment and model testing.
- **Python**: Core programming language for implementation.

---

## Applications

- **Advanced Question Answering**:
   - Provides reasoning-based answers to complex queries.
- **Decision-Support Systems**:
   - Acts as a logical engine for decision-making processes.
- **Educational Tools**:
   - Assists in solving reasoning-based problems for learning purposes.
- **AI Assistants**:
   - Enhances conversational agents by improving their reasoning capabilities.

---

## Future Scope

- **Expand the Dataset**:
   - Use larger, diverse datasets to further enhance reasoning capabilities.
- **Multi-Language Support**:
   - Extend reasoning functionality to support multiple languages.
- **Real-Time Applications**:
   - Deploy the system in real-time environments for practical use cases such as customer support or educational tools.
- **Integration with Other Models**:
   - Combine reasoning capabilities with additional LLMs for hybrid solutions.

---

## Contributors

- **Harshith Deshalli Ravi**  
  [GitHub](https://github.com/HarshithDR)

---

## Contact

For questions, suggestions, or collaboration opportunities, feel free to reach out to [Harshith Deshalli Ravi](https://github.com/HarshithDR).
